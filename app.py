from flask import Flask, render_template, request, jsonify, session
import os
import fitz  # PyMuPDF
import requests
from dotenv import load_dotenv
import json
import uuid
import shutil  # ë””ë ‰í† ë¦¬ ì´ˆê¸°í™”ìš©

load_dotenv()

app = Flask(__name__)
app.secret_key = os.getenv("FLASK_SECRET_KEY", "default_secret")

# ë©”ëª¨ë¦¬ ê¸°ë°˜ ì„¸ì…˜ìœ¼ë¡œ ë³€ê²½ (ì„œë²„ ì¬ì‹œì‘ ì‹œ ì´ˆê¸°í™”)
app.config['SESSION_TYPE'] = 'memory'  # íŒŒì¼ ì‹œìŠ¤í…œ ëŒ€ì‹  ë©”ëª¨ë¦¬ ì‚¬ìš©
app.config['SESSION_PERMANENT'] = False
app.config['SESSION_USE_SIGNER'] = True

UPLOAD_FOLDER = os.path.join("static", "uploads")
CHAT_LOG_FOLDER = "chat_logs"
CONTENT_FOLDER = os.path.join("static", "content")

# ì„œë²„ ì‹œì‘ ì‹œ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
for folder in [UPLOAD_FOLDER, CHAT_LOG_FOLDER, CONTENT_FOLDER]:
    if os.path.exists(folder):
        shutil.rmtree(folder)  # ë””ë ‰í† ë¦¬ ì™„ì „ ì‚­ì œ
    os.makedirs(folder, exist_ok=True)

def extract_text_from_pdf(filepath):
    text = ""
    with fitz.open(filepath) as doc:
        for page in doc:
            text += page.get_text()
    return text

def get_session_id():
    if "session_id" not in session:
        session["session_id"] = str(uuid.uuid4())
    return session["session_id"]

def save_content(session_id, key, content):
    filepath = os.path.join(CONTENT_FOLDER, f"{session_id}_{key}.txt")
    with open(filepath, "w", encoding="utf-8") as f:
        f.write(content)
    return filepath

def load_content(session_id, key):
    filepath = os.path.join(CONTENT_FOLDER, f"{session_id}_{key}.txt")
    if os.path.exists(filepath):
        with open(filepath, "r", encoding="utf-8") as f:
            return f.read()
    return ""

def check_ollama_status():
    url = "http://localhost:11434/api/tags"
    try:
        response = requests.get(url, timeout=5)
        return response.status_code == 200
    except requests.exceptions.RequestException:
        return False

def get_ollama_models():
    if not check_ollama_status():
        return ["Ollama server not running"]
    url = "http://localhost:11434/api/tags"
    try:
        response = requests.get(url, timeout=5)
        if response.status_code == 200:
            models = [model["name"] for model in response.json()["models"]]
            return models if models else ["No models installed"]
        else:
            return ["Ollama server error"]
    except requests.exceptions.RequestException as e:
        return [f"Ollama connection failed: {str(e)}"]

def call_local_llm(messages, model=None):
    if not check_ollama_status():
        return "âš ï¸ Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì´ì§€ ì•ŠìŠµë‹ˆë‹¤. í„°ë¯¸ë„ì—ì„œ 'ollama serve'ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”."
    url = "http://localhost:11434/api/chat"
    available_models = get_ollama_models()
    if not model or model not in available_models:
        if "No models" in available_models[0] or "Ollama" in available_models[0]:
            return f"âš ï¸ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤: {available_models[0]}. ëª¨ë¸ì„ ì„¤ì¹˜í•˜ì„¸ìš” (ì˜ˆ: 'ollama pull mistral')."
        model = available_models[0]
    payload = {
        "model": model,
        "messages": messages,
        "stream": False
    }
    try:
        response = requests.post(url, json=payload, timeout=30)
        if response.status_code == 200:
            return response.json()["message"]["content"]
        else:
            raise Exception(f"ë¡œì»¬ LLM í˜¸ì¶œ ì‹¤íŒ¨: {response.text}")
    except requests.exceptions.RequestException as e:
        return f"âš ï¸ Ollama ì—°ê²° ì‹¤íŒ¨: {str(e)}. Ollamaê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”."

@app.route("/", methods=["GET"])
def index():
    session_id = get_session_id()
    models = get_ollama_models()
    if "selected_model" not in session or session["selected_model"] not in models:
        session["selected_model"] = models[0] if models else "Ollama server not running"
    # ì„œë²„ ì¬ì‹œì‘ ì‹œ ëª¨ë“  ë°ì´í„° ì´ˆê¸°í™” ìƒíƒœë¡œ ë Œë”ë§
    return render_template(
        "index.html",
        summary=load_content(session_id, "summary"),
        chat_history=json.loads(load_content(session_id, "chat_history") or "[]"),
        models=models,
        selected_model=session.get("selected_model")
    )

@app.route("/models", methods=["GET"])
def list_models():
    return jsonify({"models": get_ollama_models()})

@app.route("/select_model", methods=["POST"])
def select_model():
    model = request.form["model"]
    available_models = get_ollama_models()
    if model in available_models:
        session["selected_model"] = model
        return jsonify({"selected_model": model})
    return jsonify({"error": "ì„ íƒí•œ ëª¨ë¸ì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}), 400

@app.route("/upload", methods=["POST"])
def upload_pdf():
    session_id = get_session_id()
    file = request.files["pdf_file"]
    if file:
        filepath = os.path.join(UPLOAD_FOLDER, file.filename)
        file.save(filepath)
        extracted_text = extract_text_from_pdf(filepath)
        save_content(session_id, "extracted_text", extracted_text)
        summary = summarize_text(extracted_text)
        save_content(session_id, "summary", summary)
        save_content(session_id, "chat_history", json.dumps([]))
        return jsonify({"summary": summary, "chat_history": []})
    return jsonify({"error": "íŒŒì¼ ì—…ë¡œë“œ ì‹¤íŒ¨"}), 400

@app.route("/reset", methods=["POST"])
def reset_chat():
    session_id = get_session_id()
    save_content(session_id, "chat_history", json.dumps([]))
    return jsonify({"chat_history": []})

@app.route("/save", methods=["POST"])
def save_chat():
    session_id = get_session_id()
    filepath = os.path.join(CHAT_LOG_FOLDER, f"{session_id}.json")
    data_to_save = {
        "chat_history": json.loads(load_content(session_id, "chat_history") or "[]"),
        "extracted_text": load_content(session_id, "extracted_text"),
        "summary": load_content(session_id, "summary"),
        "selected_model": session.get("selected_model", "No models installed")
    }
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(data_to_save, f, ensure_ascii=False, indent=2)
    return jsonify({"message": "ëŒ€í™”ì™€ ë¬¸ì„œ ë‚´ìš©ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤."})

@app.route("/load", methods=["POST"])
def load_chat():
    session_id = get_session_id()
    filepath = os.path.join(CHAT_LOG_FOLDER, f"{session_id}.json")
    if os.path.exists(filepath):
        with open(filepath, "r", encoding="utf-8") as f:
            data = json.load(f)
            save_content(session_id, "chat_history", json.dumps(data.get("chat_history", [])))
            save_content(session_id, "extracted_text", data.get("extracted_text", ""))
            save_content(session_id, "summary", data.get("summary", ""))
            session["selected_model"] = data.get("selected_model", "No models installed")
        return jsonify({
            "chat_history": data.get("chat_history", []),
            "summary": data.get("summary", ""),
            "selected_model": session["selected_model"]
        })
    return jsonify({"error": "ì €ì¥ëœ ëŒ€í™”ê°€ ì—†ìŠµë‹ˆë‹¤."}), 404

def summarize_text(text):
    if not text.strip():
        return "âš ï¸ ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
    messages = [
        {"role": "system", "content": "ë„Œ ì¡°ì„ ì‚°ì—… ê´€ë ¨ ê¸°ìˆ ë¬¸ì„œë¥¼ ì‰½ê²Œ ì„¤ëª…í•˜ëŠ” ìš”ì•½ ì „ë¬¸ê°€ì•¼. ìˆ˜í•™ ìˆ˜ì‹ì€ LaTeX í˜•ì‹(ì˜ˆ: $Q_{\\text{actual}}$)ìœ¼ë¡œ ì‘ì„±í•˜ê³ , ê°ì •ì„ ë‹´ì•„ ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•´ì¤˜ ğŸ˜„"},
        {"role": "user", "content": f"ë‹¤ìŒ ê¸°ìˆ  ë¬¸ì„œë¥¼ ì‹ ì…ì‚¬ì›ì´ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ìš”ì•½í•´ì¤˜:\n{text[:3000]}"}
    ]
    return call_local_llm(messages)

@app.route("/chat", methods=["POST"])
def chat():
    session_id = get_session_id()
    user_question = request.form["question"]
    chat_history = json.loads(load_content(session_id, "chat_history") or "[]")
    chat_history.append(("user", user_question))

    extracted_text = load_content(session_id, "extracted_text")
    if extracted_text.strip():
        messages = [
            {"role": "system", "content": "ë„Œ ì¡°ì„ ì‚°ì—… ê¸°ìˆ ë¬¸ì„œë¥¼ ì˜ ì´í•´í•˜ê³  ì„¤ëª…í•˜ëŠ” AI íŠœí„°ì•¼. ì‚¬ìš©ìê°€ ì—…ë¡œë“œí•œ ë¬¸ì„œ ë‚´ìš©ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ë©°, ìˆ˜í•™ ìˆ˜ì‹ì€ LaTeX í˜•ì‹(ì˜ˆ: $Q_{\\text{actual}} = C_d \\cdot A \\cdot \\sqrt{\\frac{2 \\cdot \\Delta P}{\\rho}}$)ìœ¼ë¡œ ì‘ì„±í•´ì¤˜. ì´ì „ ëŒ€í™” ë§¥ë½ì„ ê¸°ì–µí•˜ê³ , í•­ìƒ ë”°ëœ»í•˜ê²Œ, ì´ëª¨ì§€ì™€ í•¨ê»˜ ëŒ€ë‹µí•´ì¤˜! ğŸ›³ï¸ğŸ˜Š"},
            {"role": "system", "content": f"ğŸ“„ ì—…ë¡œë“œëœ ë¬¸ì„œ ì „ì²´ ë‚´ìš©:\n{extracted_text}"}
        ]
    else:
        messages = [
            {"role": "system", "content": "ë„Œ ì¹œì ˆí•˜ê³  ë‹¤ì¬ë‹¤ëŠ¥í•œ AI íŠœí„°ì•¼. ìˆ˜í•™ ìˆ˜ì‹ì€ LaTeX í˜•ì‹(ì˜ˆ: $Q_{\\text{actual}} = C_d \\cdot A \\cdot \\sqrt{\\frac{2 \\cdot \\Delta P}{\\rho}}$)ìœ¼ë¡œ ì‘ì„±í•˜ê³ , ì´ì „ ëŒ€í™” ë§¥ë½ì„ ê¸°ì–µí•˜ë©°, í•­ìƒ ë”°ëœ»í•˜ê²Œ, ì´ëª¨ì§€ì™€ í•¨ê»˜ ëŒ€ë‹µí•´ì¤˜! ğŸ˜Š"}
        ]

    for speaker, text in chat_history[:-1]:
        role = "user" if speaker == "user" else "assistant"
        messages.append({"role": role, "content": text})

    messages.append({"role": "user", "content": user_question})

    answer = call_local_llm(messages)
    chat_history.append(("assistant", answer))
    save_content(session_id, "chat_history", json.dumps(chat_history))
    return jsonify({"chat_history": chat_history})

if __name__ == "__main__":
    app.run(debug=True, port=5050)